#### 创建StreamingContext对象

```scala
//方式1:
通过SpartConf对象来创建
//方式2:
通过sc + 时间段来创建
val ssc = new StreamingContext(sc,Second(1))
--------------------------
具体方式:
1.
import org.apache.spark._
import ort.apache.spark.streaming._
val conf = new SparkConf().setAppName("TestDStream").setMaster("local[2]")
val ssc = new StreamingContext(conf,Seconds(1))

2. spark-shell
import org.apache.spark.streaming._
val ssc = new StreamingContext(sc,Seconds(1))
```

## 基本输入源

### 文件流

```scala
import org.apache.spark.sparkConf
import org.apache.spark.streaming.{Seconds,StreamingContext}
object StreamingWorkCount{
    def main(args:Array[String]):Unit={
     val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[*]")
     val ssc = new StreamingContext(conf,Seconds(2))
     val lines = ssc.textFileStream("file:///home/xdl/1.log/") //读取结果为:org.apache.spark.streaming.dstream.DStream[String] =... //此处DStream[String]指的是单行的格式.
     val words = lines.flatMap(_.split("\\s+"))
     val wordCounts = words.map((_.1)).reduceByKey(_+_)
     wordCounts.print()
     ssc.start()
     ssc.awaitTermination()
    }
}
```

### Socket套接字

```scala
object StreamingSocketWordCount{
    def main(args:Array[String]):Unit={
        val conf = new SparkConf().setAppName("StreamingSocketWordCount").setMaster("local[*]")
        val ssc = new StreamingContext(conf,Seconds(2))
        val sc = ssc.sparkContext
        sc.setLogLevel("WARN")
        val lines = ssc.socketTextStream("master",9999)
        val words = lines.flatMap(_.split(" "))
        val wordCounts = words.map((_,1)).reduceByKey(_+_)
        wordCounts.print()       
        ssc.start()
        ssc.awaitTermination()
    }
}
另一个终端: nc -lk 9999
```

```scala
模仿nc的数据源
object SocketLikeNC{
    def main(args:Array[String]):Unit={
        val words = "Hello World Hello Hadoop Hello spark kafka hive zookeeper hbase flume sqoop".split("\\s+")
        val n = words.length
        val port = 9999
        val random = new Random()
        
        val ss = new ServerSocket(port)
        //socket相当于管道,传输数据用流!服务器是被动的角色
        val socket = ss.accept()
        println("成功连接到本地主机:" + socket.getInetAddress)
        while(true){
            val out = new PrintWriter(socket.getOutputStream)
            out.println(words(random.nextInt(n))+" " + words(random.nextInt(n)))
            out.flush()
            Thread.sleep(400)
        }
        
    }
}
```

### RDD队列流

```scala
object RDDQueueStream {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("RDDQueueStream").setMaster("local[2]")
    // 每隔5秒对数据进行处理
    val ssc = new StreamingContext(sparkConf, Seconds(5))
    ssc.sparkContext.setLogLevel("WARN")

    val rddQueue = new Queue[RDD[Int]]()
    val queueStream = ssc.queueStream(rddQueue)
    val mappedStream = queueStream.map(r => (r % 10, 1))
    val reducedStream = mappedStream.reduceByKey(_ + _)

    reducedStream.print()
    ssc.start()

    // 每秒产生一个RDD
    for (i <- 1 to 5){
      rddQueue.synchronized {
        rddQueue += ssc.sparkContext.makeRDD(1 to 100, 2)
      }
      Thread.sleep(1000)
    }
    ssc.awaitTermination()
    //ssc.stop()
  }
}
```

## 高级输入

### Kafka

kafka可以同时满足在线实时处理和批量离线处理.

(离线处理: 消息持久化到磁盘,理论上可以长期保存,且不影响性能)

 消费者 2个版本:(zookeeper和brokelist)

- 高版本: 

  kafka-console-consumer.sh - -bootstrap-server node1:9092,node2:9092,node3:9092 - -topic mykafka1

- 低版本:

  kafka-console-consumer.sh - -zookeeper node1:2181,node2:2181,node3:2181/kafka0.11 - -topic mykafka1

用idea,程序中的api对应的是低版本!

其他常用命令

```scala
kafka-topics.sh --zookeeper node1:2181/kafka0.11 --cr eate --topic mykafka1 --partitions 5 --replication-factor 3
kafka-topics.sh --zookeeper node1:2181/kafka0.11 --list
kafka-topics.sh --zookeeper node1:2181/kafka0.11 --describe --topic mykafka1

kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092 --topic mykafka1


-------------
//获取消费者组(没有经过zookeeper的消费组)
kafka-consumer-groups.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --list

//获取消费者组(not those using the java consumer API)
kafka-consumer-groups.sh - -zookeeper node1:2181,node2:2181,node3:2181/kafka0.11 - -describe  - -group test1

小结:
0.8版本以前:
消费者API scala, zookeeper(offset),系统管理offset
0.8版本之后: 
新消费者API JAVA/broker-list offset(消息队列)(不用频繁访问zk,因为那样会影响性能) 允许自己管理offset
```

### Flume

需要启动: idea—>flume的agent —>telnet

用poll的方式取数据:

装idea的机器上: cd ~/.m2

find . -name spark-streaming-flume-sink_2.11-2.2.0.jar

## DStream有状态/无状态转换操作

ds.transform(x=>...)

transform后x为rdd.如果没有提供ds的转化操作,可以先transform转化为rdd的变换.

> 黑名单过滤

```scala
// 如果有值就原样输出,没有的话为true
a.getOrElse(true)
b.getOrElse(true)
//如果有值就原样输出,没有值的设为false
a.getOrElse(false)
b.getOrElse(false)
----------------------------------
val arr1 = Array(("spark",true),("scala",false))
val rdd1 =  sc.makeRDD(arr1)
val arr2 = Array("1 hadoop","2 spark","3 scala","4 java","5 hive")
val rdd2 = sc.makeRDD(arr2)

```

