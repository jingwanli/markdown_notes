## Spark SQL

spark sql最终跑的一定还是rdd.

![](/Users/lijingwan/Documents/img/屏幕快照 2018-05-07 下午9.25.07.png)

sql解析在整个sql运行中是最简单的: 包括语法解析,词法解析,合法性解析.

spark sql操纵的是: DataFrame 或者是 DataSet( 屏蔽了细节, 底层仍然是rdd )

RDD: 看到的是一行行的数据, 有可能被包装了,有可能没有被包装.

​	包装后看到的是对象.不包装也是对象, tuple

DF= rdd + schema

DS= rdd+case class

> 2.0以前: sparkcontext, sqlcontext, hivecontext, streamingcontext
>
> 2.0以后: sparksession(sparkcontext, sqlcontext, hivecontext), streamingcontext

```scala
val spark = SparkSession.builder.appName("test").
			enableHiveSupport().getOrCreate()
```

spark-shell中:

```scala
import org.apache.spark.sql.Row
val row1 = Row(1,1.1,"abc")
row1
//3种方式取数据 
row1(0) //结果: Any = 1

row1.getInt(0)
row1.getDouble(1)
row1.getString(2) //后面的下标要对应

row1.getAs[Int](0)
row1.getAs[Double](1)
```

```scala
//创建schema
import org.apache.spark.sql.types._
//方式1:(最后一个布尔,代表是否可以为空.缺省是true)
val schema = (new StructType).add("id","int",false).add("name","String",false).add("height","double",false)
//方式2:
//一个字段为一个StructField(相当于一个列的定义!)
val schema1 = StructType(StructField("name",StringType.false)::...::Nil) //或者List,Seq
//new StructType---> 用add方法
//直接使用StructType---> 用StructField
```

> DSL

Domain Specified Language : 领域专用语言(数据库领域,操纵的是结构化的数据)

### DataSet的创建

1. range创建dataset

   val numDS = spark.range(5,100,5) //系统会指定一个列名!

   numDS.schema //系统自动创建

   numDS.show

   numDS.orderBy("id").show(5)

   numDS.orderBy(desc("id")).show(5)

2. 从集合创建dataset

```scala
//idea中: 方式1:
import spark.implicits._ //需要加上!!

case class Person(name:String, age:Int, height:Double)

val seq1 = Seq(Person("andy",11,135), Person("tom", 12, 140 ) , Person("jack",10 ,130))
val ds1 = spark.createDataset(seq1)
ds1.show

//方式2: 不用case class
val seq1 = Seq(("andy",11,135),("tom", 12, 140 ),("jack",10 ,130));
val ds1 = spark.createDataset(seq1)
ds1.show//系统分配的列名为: _1,_2,_3
```

3. 从集合创建DF

```scala
val seq1 = Seq(("andy",11,135),("tom", 12, 140 ),("jack",10 ,130)); 

val df1 = spark.createDataFrame(seq1)

添加列名:
//方式1:
val df1 = spark.createDataFrame(seq1).withColumnRenamed("_1", "name1").withColumnRenamed("_2", "age1")withColumnRenamed("_3", "height1")
//方式2:
val df1 = spark.createDataFrame(seq1).toDF("name","age","height")
```

4. rdd转为DS/DF

```scala
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val arr = Array(("andy",11,135.0),("tom", 12, 140.0 ),("jack",10 ,130.0))//最后一个字段必须为double形式!
val rdd = sc.makeRDD(arr)
//方式1:
val df = rdd.toDF //有时转不过去,用方式2
//方式2 
val rdd1 = rdd.map(x=>Row(x._1,x._2,x._3))
val schema = StructType(List(StructField("name",StringType,false),StructField("age",IntergerType,false),StructField("height",DoubleType,false)))
val rddtoDF = spark.createDataFrame(rdd1,schema)

```

5. RDD转成DataSet / DataFrame

```scala
val rdd2 = spark.sparkContext.makeRDD(arr).map(f=>Person(f._1,f._2,f._3))
val ds2 = rdd2.toDS()
val df2 = rdd2.toDF()
//可能转不过去!
```

注: rdd是抽象类, dataset不是抽象类

6. rdd转成Dataset

   val ds3 = spark.createDataset(rdd2) //最好rdd2里要是case class

   ds3.show(10)

   //转成DF的话,除了create处改变外,参数中还要加一个schema

7. 读取文件

```scala
val df4 = spark.read.csv("file:///home/spark/t01.csv")
df4.show()

//不指定schema,读取到的是String
//返回值: DF
```

![](/Users/lijingwan/Documents/img/屏幕快照 2018-05-08 上午12.13.20.png)

### 相关操作

读文件:相当于transformation(new 了一个新df)

保存文件: 相当于action (触发了一个动作)

#### Transformation

注: pyhthon是不能用ds的,只能用到df

读文件时: option中的字段写错了,如inferschema111,不会报任何错, 但是不生效(此处为自动类型推断)!

#### action

```scala
df1.count
df1.show //缺省显示20行
df1.show(1000,false) //false表示不截断! 看全显示,1000表示1000行
df1.collect //返回的Row数组
df1.collectAsList//返回的Row的List
ds1.collect //返回的是数组,但不是row,是class
spark.catalog.listFunctions —> 返回的是dataset!

df1.take(10) //取回的是array
df1.first
df1.head(2)
df1.takeAsList(2) //取回的是List

case class Person(name:String,age:Int,height:Int)
val seq1 = Seq(Person("Jack",28,184),Person("Tom",10,144),Person("Andy",16,165))
ds1.reduce((x,y)=>Person("sum",x.age+y.age,x.height+y.height))
```

##### 结构属性

```scala
df1.columns //返回带字段的array
df1.dtypes //返回array 包含字段和字段类型
df1.explain// 打印df的执行计划 包含逻辑执行计划和物理执行计划
df1.col("ename") // 拿到列对象
df1.printSchema //列出整个结构
```

#### Transformation

```scala
1. 
import spark.implicits._
df1.map(x=>x(0)).show //df map后为ds! x是Row类型

2.
case class Peoples(age:Int,names:String)
val seq1 = Seq(Peoples(30,"Tom,Andy,Jack"),Peoples(20,"Pand,Gate,Sundy"))
val ds = spark.createDataset(seq1)
//x就是Peoples
val ds2 = ds.flatMap(x=>{
       val age = x.age
    //y是names数组,map相当于循环(输入时数组,输出也是数组),flatmap一定是将类似集合的东西给压平
       val people = x.names.split(",").map(y=>Peoples(age,y))
       people
})

3.
val df2 = df1.randomSplit(Array(0.3,0.4,0.5))
//结果是Array里面套DS
df2.size //3
df2(0)
df2(1)
df2(2)
df2(0).count //4
//rdd上也可以用
val arrRDD = df1.rdd.randomSplit(Array(0.3,0.4,0.3))

4.
val df2 = df1.limit(5) //结果为ds(内部结构为Row)

5.
distinct
df1.union(df1).distinct.count //distinct返回要么ds,要么df,是以行位单位进行的去重

6.
dropDuplicates
df1.dropDuplicates("deptno")
//指定行,count后为3. 如果show的话,前面的数据是随机的.
df1.dropDuplicates("mgr","job").count //针对2行

7.
describe()
df1.describe().show //求count,mean,max...
```

#### 存储相关的方法

//与RDD一致

```scala
1.import org.apache.spark.storage.StorageLevel
df1.persist(StorageLevel.MEMORY_ONLY)

2.spark.sparkContext.setCheckpointDir("hdfs://master:8020/checkpoint")
df1.checkpoint()

3.df1.cache()

4.df1.unpersist(true)//解除持久化

5.df1.count()
```

#### select相关

```scala
列的5种表示方法: " ", $" ", ' , col( ), ds(" ") / df(" ")
注意: 不要混用, 必要时使用spark.implicits._ 

df1.select('ename,'sal+100)
df1.select('sal).show
//可以用这两种写法!而且ename也要加$! 因为没有"sal+100"这个列!
df1.select($"ename", $"sal"+100).show

//可以使用expr表达式!(expr可以做复杂一些的事情,里面只能用引号!) expr是一个内置函数
//方式1
df1.select(expr("comm+100"),expr("sal+100"),expr("ename")).show
//方式2
df1.selectExpr("ename as name").show
df1.selectExpr("pow(sal,2)").show

//删除列
val df2 = df1.drop("comm")
df2.printSchema
val df2 = df1.drop("comm","sal") //必须分开加引号!

//修改列值
df2.withColumn("sal",$"sal"+100)

//修改列名
df2.withColumnRenamed("sal","newsal")

//过滤
df1.where("sal>1000").show
df1.filter("sal>1000").show
df1.filter($"sal">1000).show
/////备注:drop,withColumn,withColumnRenamed返回的是DF

//cast 类型转换
//方式1
df1.selectExpr("cast(empno as string)").printSchema
//方式2
import org.apache.spark.sql.types._
df1.select('empno.cast(StringType)).printSchema


```

#### groupBy

```scala
df1.groupBy("deptno").count().show//返回2列
df1.groupBy("deptno").max().show//返回多列
df1.groupBy("deptno").sum().show//返回多列 

df1.groupBy("deptno").agg("sal"->"sum","sal"->"count").show
df1.groupBy("deptno").agg(max("sal"),count("sal")).show
//加修改列名
df1.groupBy("deptno").agg(max("sal").as("max1"),count("sal").as("count1")).show
//也可以: withColumnRenamed-->来取别名
```

#### orderBy, sort相关

```scala
df1.orderBy("sal").show
df1.orderBy($"sal" desc).show
df1.orderBy($"sal".desc).show
df1.orderBy(col("sal").desc).show
df1.orderBy(-'sal).show
//2次排序
df1.orderBy(-'sal,'deptno).show

```

## DSL

聚合函数(其他类型) over (partition by … order by …rows/range between … and ...)

```scala
val df = spark.read.option("header","true").
option("inferschema","true").csv("data/analyfuncdata.csv")

import org.apache.spark.sql.expressions.Window
val w1 = Window.partitionBy("cookieid").orderBy("createtime")
val w2 = Window.partitionBy("cookieid").orderBy("pv")

//聚合
df.select($"cookieid",$"createtime",$"pv",sum("pv").over(w1).alias("pv1")).show

df.select($"cookieid",$"createtime",$"pv",sum("pv").over(w1) as("pv2")).show

//排名
df.select($"cookieid",$"pv",row_number.over(w2).alias("rownumber")).show
//lag
df.select($"cookieid",$"pv",lag("pv",2).over(w2).alias("rownumber")).show
//cube rollup
df.cube("id","name").agg(sum("num")).show //这里cube可以看做group by
df.rollup("id","name").agg(sum("num")).show 
```

##合并记录

内置函数:

concat_ws: 实现多行记录合并成一行

collect_set:对记录去重

collect_list: 不对记录去重

```scala
  case class UserAddress(userid:String,address:String)
val userinfo = Seq(UserAddress("a","address1"),UserAddress("a","address2"),UserAddress("a","address2"),UserAddress("b","address3"),UserAddress("c","address4"))
val ds1 = spark.createDataset(userinfo)
ds1.createOrReplaceTempView('t1)
spark.sql("""
	select userid,concat_ws("%",collect_set(address)) as add1Set, concat_ws("%",collect_list(address)) as add2List
		from t1 group by userid
""").show
//这里必须要用group by!!!!!

//DSL语法
 ds1.groupBy($"userid").agg(concat_ws(";",collect_set("address")) as "address"
).show //as后面也必须加引号!!
                         
```

## 展开记录

```scala
val ds2 = ds1.groupBy($"userid").agg(collect_set($"address") as "address") 
//此处不能用concat_ws!!!collect_set结果是array
//concat_ws后,是字符串!!!没办法展开!需要先做split才能展开!

//Sql
ds2.createOrReplaceTempView("t2")
//explode: 将一行中复杂的array或者map结构拆分成多行
spark.sql("select userid,explode(address) from t2").show

//DSL语法
ds2.select($"userid",explode($"address")).show
```





