---
typora-root-url: ../img
---

## 目录池

```java
# Name the components on this agent
# agent1:代理的名字
agent1.sources = source1
agent1.channels = channel1
agent1.sinks = sink1

# Describe/configure the source 
# spooldir 目录池方式
agent1.sources.source1.type = spooldir 目录池
agent1.sources.source1.spoolDir=/home/xdl/flume/aboutyunlog 此目录要自己建!
#配置往channel1传输数据
agent1.sources.source1.channels = channel1
agent1.sources.source1.fileHeader=false

# Describe the sink
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path=hdfs://master:9000/flume/aboutyunlog 此目录是自动创建!
agent1.sinks.sink1.hdfs.fileType=DataStream
agent1.sinks.sink1.writeFormat=TEXT
# hdfs sink间隔多长将临时文件滚动成最终目标文件，单位：秒；
agent1.sinks.sink1.hdfs.rollInterval=4
#配置从channel1接收数据
agent1.sinks.sink1.channel = channel1

# Use a channel which buffers events inmemory
agent1.channels.channel1.type = file
agent1.channels.channel1.checkpointDir=/home/xdl/flume/aboutyun_check
agent1.channels.channel1.dataDirs=/home/xdl/flume/aboutyun_data
```

## avro

1. conf中 写vim avro1.conf

```java
a1.sources=r1
a1.channels=c1
a1.sinks=s1

a1.sources.r1.type=avro
a1.sources.r1.channels=c1
a1.sources.r1.bind=master
a1.sources.r1.port=4141

a1.sinks.s1.type=logger //如果是要传入下一个agent,则这里写avro
a1.sinks.s1.channel=c1

a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

```

2. 在apache-flume-1.6.0-bin中写bash

   vim avro.bash

   bin/bash
   bin/flume-ng agent -n a1 -c conf -f conf/avro1.conf -Dflume.root.logger=DEBUG,co
   nsole

3. 创建指定的文件

   avro: 可以把一个文件通过avro序列化框架序列化输出到sink指定的log中

   echo "hello world" > /home/xdl/apache-flume-1.6.0-bin/log1.00 (不用提前创建)

   //执行后就会看到创建好的: log.00  内容是:"hello world"

4. 运行flume :  bash avro.bash

5. 使用avro-client发送文件

   How将文件发给flume框架?

   bin/flume-ng agent -n a1 -c ./conf -H master -p 4141 -F /home/xdl/apache-flume-1.6.0-bin/log1.00

6. 命令行中打印结果

   注意 bash avro.bash 后,重新打开一个客户端运行avro-client

   然后在之前的窗口可以看到打印出的 hello world

## Syslogtcp

Syslogtcp监听TCP的端口做为数据源

1. conf中创建agent配置文件 syslog_tcp.conf

   ```java
   a1.sources=r1
   a1.channels=c1
   a1.sinks=s1

   a1.sources.r1.type=syslogtcp
   a1.sources.r1.host=master
   a1.sources.r1.port=5140
   a1.sources.r1.channels=c1

   a1.sinks.s1.type=logger
   a1.sinks.s1.vichannel=c1

   a1.channels.c1.type=memory
   a1.channels.c1.capacity=1000
   a1.channels.c1.transactionCapacity=100

   ```

2. 在apache-flume-1.6.0-bin中写bash:syslog_tcp.bash

   ```java
   #!/bin/bash
   bin/flume-ng agent -n a1 -c conf -f conf/syslog_tcp.conf -Dflume.root.logger=DE
   BUG,console>syslog_tcp.log 2>&1 &                                 
   ```

3. 启动flume

   bash syslog_tcp.bash

4. 向flume发数据 利用nc网络工具

   先nc一下,如果command not found: —>安装nc(root下)

   法1. yum install -y nc (需要上网!)

   法2. 将安装包拷贝至一个目录下

   ​	运行: rpm -ivh /home/xdl/nc*

5. echo "hello world" | nc master 5140

6. ​cat syslog_tcp.log


## Flume集群

master: 汇聚节点

slave1, slave2: 采集节点

将3台节点都安装flume

####1. 写conf

#####汇聚节点

master:conf下 vim agent1.conf

```java
agent1.channels=channel1
agent1.sources=source1
agent.sinks=sink1

agent1.sources.source1.channels=channel1
agent1.sources.source1.type=avro
agent1.sources.source1.bind=master
agent1.sources.source1.port=5140
agent1.sources.source1.threads=20

agent1.channels.channel1.type=memory
agent1.channels.channel1.capacity=100000
agent1.channels.channel1.transactionCapacity=100
agent1.channels.channel1.keep-alive=30

agent1.sinks.sink1.channel=channel1
agent1.sinks.sink1.type=hdfs
agent1.sinks.sink1.hdfs.path=hdfs://master:9000/flume/data/%Y-%m-%d (是个目录)
agent1.sinks.sink1.hdfs.filePrefix=data
agent1.sinks.sink1.hdfs.useLocalTimeStamp=true
agent1.sinks.sink1.hdfs.inUseSuffix=.tmp
agent1.sinks.sink1.hdfs.writeFormat=Text
agent1.sinks.sink1.hdfs.fileType=DataStream
agent1.sinks.sink1.hdfs.rollInteval=300
agent1.sinks.sink1.hdfs.rollSize=0 //大小不限制
agent1.sinks.sink1.hdfs.rollCount=0 //个数不限制
agent1.sinks.sink1.hdfs.batchSize=10 //批处理
agent1.sinks.sink1.txtEvenMax=1000//一次性能处理的最大events的个数
agent1.sinks.sink1.hdfs.callTimeout=60000 //连接到hdfs的超时时间
agent1.sinks.sink1.hdfs.appendTimeout=60000 //写数据到hdfs的超时时间

```

#####采集节点

slave1:

```java
agent2.sources=source2
agent2.channels=channel2
agent2.sinks=sink2

agent2.sources.source2.type=syslogtcp
agent2.sources.source2.bind=slave1
agent2.sources.source2.port=5140
agent2.sources.source2.encoding=utf-8

agent2.sinks.sink2.type=avro
agent2.sinks.sink2.hostname=master
agent2.sinks.sink2.port=5140

agent2.channels.channel2.type=memory
agent2.channels.channel2.capacity=100000
agent2.channels.channel2.transactionCapacity=100
agent2.channels.channel2.keep-alive=30

agent2.sources.source2.channels=channel2
agent2.sinks.sink2.channel=channel2
```

slave2

```java
agent3.sources=source3
agent3.channels=source3
agent3.sinks=sink3

agent3.sources.source3.type=syslogtcp
agent3.sources.source3.bind=slave2
agent3.sources.source3.port=5140
agent3.sources.source3.encoding=utf-8

agent3.channels.channel3.type=memory
agent3.channels.channel3.capacity=100000
agent3.channels.channel3.transactionCapacity=100
agent3.channels.channel3.keep-alive=30

agent3.sinks.sink3.type=avro
agent3.sinks.sink3.hostname=master
agent3.sinks.sink3.port=5140

agent3.sources.source3.channels=channel3
agent3.sinks.sink3.channel=channel3
```

#### 2.写bash

##### 先写汇聚节点

bin下: vim agent1.bash

```java
#!bin/bash
bin/flume-ng agent -n agent1 -c conf -f conf/agent1.conf -Dflume.root.logger=
DEBUG,console>./agent1_test.log 2>&1 &                                                 
```

##### 采集结点1:slave1

bin下:vim agent2.bash

```java
#!bin/bash
bin/flume-ng agent -n agent2 -c conf -f conf/agent2.conf -Dflume.root.logger=
DEBUG,console>./agent2_test.log 2>&1 &
```

##### 采集结点2:slave2

```java
#!bin/bash
bin/flume-ng agent -n agent3 -c conf -f conf/agent3.conf -Dflume.root.logger=
DEBUG,console>./agent3_test.log 2>&1 &
```

#### 3.eclipse —> ProductData.java

实际生产中,上千台集群,秒级的数据量可以到达PB级别, —> 首先落地到hdfs的一个目录下,

Storm读目录,只要有新数据加入, 就读出放入spout, spout和bolt进行处理,将处理结果写入关系型数据库中,然后通过web前端的java框架, 将最终结果通过图形化的web方式展示出来.

 #### 4.启动flume

先启动汇聚节点!

bin下: bash agent1.bash

cat agent1_test.log  查看有无错误

启动采集节点:

bin下: bash agent2.bash -> cat agent2_test.log  

​	   bash agent3.bash ->cat agent3_test.log  

##### 5. 查看数据

hadoop fs -ls /flume/data

![](/屏幕快照 2018-03-31 下午1.06.48.png)

//每5分钟生成一个文件(conf里配置), 一旦生成文件,.tmp文件就没有了.

![](/屏幕快照 2018-03-31 下午1.14.13.png)

















