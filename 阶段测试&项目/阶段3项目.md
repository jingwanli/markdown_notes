###实时监测共享账户

1. 数据生成器生成数据

2. 启动flume(bash启动), 接收数据——>存储到hdfs上

3. 代码部分: 

   ​

   ProductData.java


```java
import java.io.BufferedOutputStream;
import java.io.IOException;
import java.net.DatagramPacket;
import java.net.DatagramSocket;
import java.net.InetAddress;
import java.net.Socket;
import java.net.UnknownHostException;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;

public class ProductData {
	public static long userTime;//collecttime
	public static String userAccount;
	public static String[] userIP1 = { "223", "175", "202", "110", "221",
			"103", "118", "125" };
	public static String[] userIP2 = { "220", "184", "100", "166", "207", "22",
			"213", "72" };
	public static String[] userIP3 = { "0", "128", "255", "191", "135", "100",
			"63", "103", "144", "159", "136", "143" };
	public static String[] userIP4 = { "0", "255" };
	public static int skypeid;
	public static String innerIP;
	public static String privateValue;
	public static String[] devName1 = { "MacBook Pro mac", "iphone 5s",
			"Thinkpad" };
	public static String[] osName1 = { "os x 10.9", "ios", "win 8" };
	public static String str;
	public static String data;
	public static int skypeid1;
	public static String natIPTest;

	/**
	 * gen MD5 for string.
	 */
	public String md5s(String plainText) {
		try {
			MessageDigest md = MessageDigest.getInstance("MD5");
			md.update(plainText.getBytes());
			byte b[] = md.digest();
			int i;
			StringBuffer buf = new StringBuffer("");
			for (int offset = 0; offset < b.length; offset++) {
				i = b[offset];
				if (i < 0)
					i += 256;
				if (i < 16)
					buf.append("0");
				buf.append(Integer.toHexString(i));
			}
			str = buf.toString();
			// System.out.println(buf.toString());// 32 bit encryption
			// System.out.println("result: " + buf.toString().substring(8,
			// 24));// 16 bit encryption
		} catch (NoSuchAlgorithmException e) {
			e.printStackTrace();
		}
		return str;
	}

	/**
	 * create random array.
	 */
	public String getDev() {
		int index = (int) (Math.random() * devName1.length);
		return devName1[index];
	}

	public String getOs() {
		int index = (int) (Math.random() * osName1.length);
		return osName1[index];
	}

	public String getIP1() {
		int index = (int) (Math.random() * userIP1.length);
		return userIP1[index];
	}

	public String getIP2() {
		int index = (int) (Math.random() * userIP2.length);
		return userIP2[index];
	}

	public String getIP3() {
		int index = (int) (Math.random() * userIP3.length);
		return userIP3[index];
	}

	public String getIP4() {
		int index = (int) (Math.random() * userIP4.length);
		return userIP4[index];
	}

	public int getSkypeID() {
		skypeid1 = new Random().nextInt(999999999);
		return skypeid1;
	}

	public String getInnerIP() {
		java.util.Random rd = new java.util.Random();
		return rd.nextInt(255) + "." + rd.nextInt(255);
	}

	/**
	 * create data
	 */
	public static List<String> creatData() {
		
		ProductData pd = new ProductData();
		Random rd = new Random((long) (Math.random() * 75));
		List<String> dd = new ArrayList<String>();
		userTime = System.currentTimeMillis();
		innerIP = "172" + "." + "16" + "." + pd.getInnerIP();
		String userIP = pd.getIP1() + "." + pd.getIP2() + "." + pd.getIP3()
				+ "." + pd.getIP4();
		String devName = pd.getDev();
		String osName = pd.getOs();
		userAccount = "9" + rd.nextInt(9999999);
		if (userAccount.length() < 12) {
			String userModel = "1234567890";
			int a = 11 - userAccount.length();
			String userResult = userModel
					.substring((int) Math.random() * 10, a);
			userAccount = userAccount + userResult;
		}
		int times = rd.nextInt(25) + 1;
		skypeid = pd.getSkypeID();
		for (int i = 0; i < times; i++) {
			int s = rd.nextInt(3);
			if (s == 2) {
				skypeid = pd.getSkypeID();
				innerIP = "172" + "." + "16" + "." + pd.getInnerIP();
				privateValue = pd.md5s(userAccount);
			}
			String data = userTime + "\t" + userAccount + "\t" + userIP + "\t"
					+ skypeid + "\t" + innerIP + "\t" + privateValue + "\t" + devName
					+ "\t" + osName + "\n";
			dd.add(data);
		}
		return dd;
	}

	/**
	 * send socket-UDP
	 */

	public void udpSendSocket() {
		DatagramSocket ds;
		try {
			ds = new DatagramSocket();
			List<String> dataflumn = ProductData.creatData();
			// System.out.println(dataflumn);
			for (String list : dataflumn) {
				;
				byte[] by = list.getBytes();
				DatagramPacket dp = new DatagramPacket(by, by.length,
						InetAddress.getByName("172.16.185.199"), 10006);
				ds.send(dp);
			}
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	/**
	 * send socket-TCP
	 */
	public void tcpSendSocket() {
		Socket s1 = null;
		try {
			List<String> dataflumn = ProductData.creatData();
			// System.out.println(dataflumn);
			for (String list : dataflumn) {
				//s1 = new Socket("192.168.2.101", 10034);
				s1 = new Socket("172.16.185.200",5140);
				BufferedOutputStream bosout = new BufferedOutputStream(
						s1.getOutputStream());
				bosout.write(list.getBytes());
				bosout.flush();
			}
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			if (null != s1) {
				try {
					s1.close();
				} catch (IOException e) {
					e.printStackTrace();
				}
			}
		}
	}
	
	
	/**
	 * send socket-TCP
	 */
	public void tcpSendSocket2() {
		Socket s1 = null;
		try {
			List<String> dataflumn = ProductData.creatData();
			// System.out.println(dataflumn);
			for (String list : dataflumn) {
				//s1 = new Socket("192.168.2.101", 10034);
				s1 = new Socket("172.16.185.201",5140);
				BufferedOutputStream bosout = new BufferedOutputStream(
						s1.getOutputStream());
				bosout.write(list.getBytes());
				bosout.flush();
			}
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			if (null != s1) {
				try {
					s1.close();
				} catch (IOException e) {
					e.printStackTrace();
				}
			}
		}
	}
	

	public static void main(String[] args) throws UnknownHostException {
		
		
		/*List<String> list = ProductData.creatData();
		for(String ss : list){
			System.out.println(ss);
		}*/
		Thread thread = new Thread(new SleepRunner());
		thread.start();
		Thread thread2 = new Thread(new SleepRunner2());
		thread2.start();

	}
}

/**
 * set rate of transform
 */
class SleepRunner implements Runnable {

	private ProductData pd = new ProductData();

	@SuppressWarnings("static-access")
	@Override
	public void run() {
		while (true) {
			try {
				pd.tcpSendSocket();
				Thread.currentThread().sleep((long) (Math.random() * 1000));
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
	}
}

/**
 * set rate of transform
 */
class SleepRunner2 implements Runnable {

	private ProductData pd = new ProductData();

	@SuppressWarnings("static-access")
	@Override
	public void run() {
		while (true) {
			try {
				pd.tcpSendSocket2();
				Thread.currentThread().sleep((long) (Math.random() * 1000));
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
	}
}


```

ReadDataSpout1.java

```java
package test;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.URI;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;

import backtype.storm.spout.SpoutOutputCollector;
import backtype.storm.task.TopologyContext;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseRichSpout;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;

public class ReadDataSpout1 extends BaseRichSpout{
	
	private Map conf;
	private SpoutOutputCollector collector;
	private InputStream in = null;
	private FileSystem fs;
	private String uri;
	private Date date;
	private String dateStr;
	private String dayStr;
	private String hourStr;
	private SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd-HH");
	private Path[] paths; 
	private String str; //data read
	
	

	@Override
	public void nextTuple() {
		try {
			read();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	@Override
	public void open(Map conf, TopologyContext arg1, SpoutOutputCollector collector) {
		
		this.conf = conf;
		this.collector = collector;
	}
	
	public Path[] getPaths() throws IOException{
		date = new Date();
		dateStr = sdf.format(date);
		dayStr = dateStr.substring(0, 10);
		hourStr = dateStr.substring(11,13);
		uri = "hdfs://master:9000/flume/data/" + dayStr;
		fs = FileSystem.get(URI.create(uri), new Configuration());
		FileStatus[] status = fs.listStatus(new Path(uri));
		paths = FileUtil.stat2Paths(status);
		//num = paths.length;
		return paths;
	}
	
	public void read() throws IOException{
		int index = getPaths().length-2;
		if(index>=0){
			String content;
			in = fs.open(new Path(paths[index].toString()));
			BufferedReader br = new BufferedReader(new InputStreamReader(in));
			while((content = br.readLine())!=null){
				str =  content + "\t" + dayStr + "\t" + index + "\t" + hourStr;
				collector.emit(new Values(str));
			}
			collector.emit(new Values("over"));
			if(index!= getPaths().length -2){
				read();
			}
		}
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		declarer.declare(new Fields("word1"));
	}

}

```

HandleBolt1.java

```java
package test;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

import backtype.storm.topology.BasicOutputCollector;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseBasicBolt;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Tuple;
import backtype.storm.tuple.Values;

public class HandleBolt1 extends BaseBasicBolt {

	/**
	 * 
	 */
	/*
	 * private boolean flag1 = true; private boolean flag2 = true; private
	 * boolean flag3 = true;
	 */
	private static final long serialVersionUID = 1L;
	private Map<String, Set<String>> map1 = new HashMap<String, Set<String>>();
	private Map<String, Set<String>> map2 = new HashMap<String, Set<String>>();
	private Map<String, Set<String>> map3 = new HashMap<String, Set<String>>();

	private Set<String> s1 = new HashSet<String>();
	private Set<String> s2 = new HashSet<String>();
	private Set<String> s3 = new HashSet<String>();

	/*
	 * private Set<String> set1 = null; private Set<String> set2 = null; private
	 * Set<String> set3 = null;
	 */
	@Override
	public void execute(Tuple input, BasicOutputCollector collector) {
		String sentence = input.getString(0);
		if ("over".equals(sentence)) {
			map1.clear(); // natIp
			map2.clear(); // qqId
			map3.clear(); // large
			s1.clear();
			s2.clear();
			s3.clear();
		}
		String[] words = sentence.split("\t");
		if (words.length == 11) {
			// words[1] words[8]:dir words[9]:mark words[10]:hour
			String userId = words[1] + "\t" + words[8] + "\t" + words[9] + "\t" + words[10];
			String natIp = words[4];
			String qqId = words[3];
			String large = words[5] + words[6] + words[7];

			Set<String> set1 = null;
			Set<String> set2 = null;
			Set<String> set3 = null;

			if (!s1.contains(userId)) {
				if ((set1 = map1.get(userId)) != null) {
					set1.add(natIp);
					map1.put(userId, set1);
					if (set1.size() == 5) {
						s1.add(userId);
						collector.emit(new Values(userId + "\t" + "natIp"));
					}
				} else {
					set1 = new HashSet<String>();
					set1.add(natIp);
					map1.put(userId, set1);
				}
			}
			if (!s2.contains(userId)) {
				if ((set2 = map2.get(userId)) != null) {
					set2.add(qqId);
					map2.put(userId, set2);
					if (map2.size() == 20) {
						s2.add(userId);
						collector.emit(new Values(userId + "\t" + "qqId"));
					}
				} else {
					set2 = new HashSet<String>();
					map2.put(userId, set2);
				}
			}
			if (!s3.contains(userId)) {
				if ((set3 = map3.get(userId)) != null) {
					set3.add(large);
					map3.put(userId, set3);
					if (map3.size() == 5) {
						s3.add(userId);
						collector.emit(new Values(userId + "\t" + "large"));
					}
				}else {
					set3 = new HashSet<String>();
					map3.put(userId, set3);
				}
			}
		}
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		declarer.declare(new Fields("word2"));
	}

}

```

HandleBolt2.java

```java
package test;

import java.sql.DriverManager;
import java.sql.SQLException;
import java.util.HashMap;
import java.util.Map;

import com.mysql.jdbc.Connection;
import com.mysql.jdbc.PreparedStatement;

import backtype.storm.task.TopologyContext;
import backtype.storm.topology.BasicOutputCollector;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseBasicBolt;
import backtype.storm.tuple.Tuple;

public class HandleBolt2 extends BaseBasicBolt {

	/**
	 * 
	 */
	private static final long serialVersionUID = 1L;
	private PreparedStatement ps;
	private Connection conn;
	private String username = "hadoop";
	private String password = "hadoop";
	private String url = "jdbc:mysql://172.16.185.199:3306/monitorproject";
	private String driver = "com.mysql.jdbc.Driver";
	private Map<String, Integer> map = new HashMap<String, Integer>();

	@Override
	public void prepare(Map stormConf, TopologyContext context) {
		try {
			Class.forName(driver);
			conn = (Connection) DriverManager.getConnection(url, username, password);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	@Override
	public void execute(Tuple input, BasicOutputCollector collector) {
		String str = input.getString(0);
		String[] all = str.split("\t",-1);
		String uid = all[0];
		String date = all[1];
		int times = Integer.parseInt(all[2]);
		String hour = all[3];
		String tag = all[4];

		String sql = "insert into abnormal(uid,date,times,hour,tag) values(?,?,?,?,?)";
		try {
			ps = (PreparedStatement) conn.prepareStatement(sql);
			ps.setString(1, uid);
			ps.setString(2, date);
			ps.setInt(3, times);
			ps.setString(4, hour);
			ps.setString(5, tag);
			ps.executeUpdate();
		} catch (SQLException e) {
			e.printStackTrace();
		}
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		
	}

}
```

Main1.java

```java
package test;

import backtype.storm.Config;
import backtype.storm.LocalCluster;
import backtype.storm.StormSubmitter;
import backtype.storm.generated.AlreadyAliveException;
import backtype.storm.generated.InvalidTopologyException;
import backtype.storm.topology.TopologyBuilder;

public class Main1 {

	public static void main(String[] args) throws AlreadyAliveException, InvalidTopologyException {
		TopologyBuilder builder = new TopologyBuilder();
		builder.setSpout("readspout", new ReadDataSpout1());
		builder.setBolt("handlebolt1", new HandleBolt1_copy()).shuffleGrouping("readspout");
		builder.setBolt("handlebolt2", new HandleBolt2()).shuffleGrouping("handlebolt1");
		Config config = new Config();
		config.setDebug(true);
		if(args!=null && args.length>0){
			config.setNumWorkers(3);
			StormSubmitter.submitTopologyWithProgressBar(args[0], config, builder.createTopology());
		}else{
			config.setNumWorkers(3);
			LocalCluster local = new LocalCluster();
			local.submitTopology("test", config, builder.createTopology());
		}
	
	}

}
```



柱状图显示:

1. 启动zookeeper
2. 本地环境(不用启动storm)
3. 启动flume采集. master 汇聚节点 bash启动(3个节点)

Flume必须有汇聚节点. 如果采集节点直接落地到hdfs的话,hdfs访问量太大,可以有多个汇聚节点.

kafka里的日志有2种,一种是数据日志(tmp…..)

一种是系统运行日志.

